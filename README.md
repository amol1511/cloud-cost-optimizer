# Cloud Cost Optimization Tool

A lightweight **Streamlit** app that analyzes multi-cloud cost & usage data (AWS, Azure, GCP or custom)
and generates **rightsizing and waste-reduction recommendations** with estimated savings.

## âœ¨ Features

- Upload a CSV (or use the provided sample) with costs and utilization metrics.
- Visualize **total spend**, **service breakdown**, and **top costly resources**.
- Detect **idle** and **underutilized** compute resources (e.g., EC2/VMs) with clear thresholds.
- **Rightsizing suggestions** (e.g., m5.large â†’ m5.medium) with estimated monthly savings.
- Tag-aware insights (e.g., costs by `env`, `owner`, `project` if tags are present).
- Exportable recommendations as CSV.

> Works offline on CSVsâ€”no cloud credentials required. You can later hook it to AWS/Azure/GCP SDKs if desired.

## ðŸ“¦ Project Structure

```
cloud-cost-optimizer/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample_data.csv
â”œâ”€â”€ streamlit_app.py
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ rules.py
    â”œâ”€â”€ loaders.py
    â””â”€â”€ analyzer.py
```

## ðŸ§ª Sample Data

`sample_data.csv` includes synthetic entries across AWS, Azure, and GCP with columns:

- `provider` â€“ one of `aws|azure|gcp|other`
- `service` â€“ e.g., `EC2`, `RDS`, `S3`, `VM`, `Compute Engine`
- `resource_id` â€“ unique name or ARN/ID
- `region` â€“ region/zone name
- `tags` â€“ json-like string or `key=value;key=value`
- `month` â€“ ISO month like `2025-08`
- `hours_running` â€“ hours used / provisioned in the month
- `cpu_avg` â€“ average CPU % (0â€“100)
- `mem_avg` â€“ average memory % (0â€“100) if available
- `cost_usd` â€“ monthly blended cost in USD
- `last_access_days` â€“ for storage objects (optional), days since last access
- `storage_gb` â€“ for storage services (optional)

You can add your own columns; unknown columns are ignored.

## ðŸš€ Quickstart

1. **Install deps** (Python 3.9+ recommended):
   ```bash
   pip install -r requirements.txt
   ```

2. **Run the app**:
   ```bash
   streamlit run streamlit_app.py
   ```

3. **Upload your CSV** or click **Use sample dataset**.

## ðŸ§  How recommendations work (high level)

- **Idle compute**: `cpu_avg < 5%` and `hours_running > 100` and `cost_usd > 5` â†’ **Stop/Terminate** suggestion.
- **Underutilized compute**: `5% â‰¤ cpu_avg < 20%` â†’ **Rightsize down one tier**.
- **Storage**: If `last_access_days > 30` and `storage_gb > 0` â†’ suggest colder tier (e.g., S3 IA / Archive).
- **High-cost outliers**: Top 10% most expensive resources flagged for review.

Savings estimates are conservative:
- Rightsizing: assumes ~35% cost reduction for one-size down.
- Idle stop: assumes 90% savings (retain minimal costs).
- Storage tiering: assumes 40% reduction on flagged storage.

Thresholds are configurable in the sidebar.

## ðŸ”Œ Bring your own exports

Export cost & usage from:
- **AWS** Cost Explorer / CUR + CloudWatch metrics
- **Azure** Cost Management + Insights / Metrics
- **GCP** Billing Export + Monitoring metrics

Map your columns to the tool's expected names or rename headers.

## ðŸ§¯ Notes & Limitations

- This tool uses heuristics; validate before acting in production.
- Streamlit charts use **matplotlib** to keep dependencies minimal.
- No cloud API calls by default. Safe to run locally/offline.

## ðŸ“„ License

MIT
